{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208e2c82",
   "metadata": {},
   "source": [
    "# Ensemble Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40f5d5",
   "metadata": {},
   "source": [
    "## 1. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182c43b",
   "metadata": {},
   "source": [
    "Ensemble learning is a machine learning technique where multiple models are trained on a dataset to make predictions, and the **predictions of those models are combined to produce a more accurate and robust prediction** than any of the individual models. In other words, ensemble learning is about combining the predictions of several weaker models to create a stronger model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c31b7",
   "metadata": {},
   "source": [
    "## 2. Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ff154",
   "metadata": {},
   "source": [
    "### Simple Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed081d86",
   "metadata": {},
   "source": [
    "#### 1. Max Voting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52b488",
   "metadata": {},
   "source": [
    "**Definition :** The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. **The predictions which we get from the majority of the models are used as the final prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cbade15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read the text file into a pandas dataframe\n",
    "df = pd.read_csv(\"/Users/crystal/Desktop/Random Forest/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d813a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics as st\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "216494be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DATASET\n",
    "x = df.drop('target', axis = 1)\n",
    "y = df['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5c54d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODELS CREATION\n",
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ceb26",
   "metadata": {},
   "source": [
    "The final prediction is stored in a numpy array called final_pred which is initialized as an empty array using the np.array([]) function. The code then runs a for loop from 0 to the length of the x_test variable (which represents the test dataset). Inside the loop, the mode function from the statistics library is used to calculate the mode of the three model predictions for each observation in the test dataset. These three model predictions are stored in pred1, pred2, and pred3. **The mode function returns the most common prediction value among the three predictions.** The resulting mode prediction is appended to the final_pred array using the np.append function. Finally, the final_pred array is printed using the print function to show the mode predictions for each observation in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa0ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "# FINAL_PREDICTION\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, st.mode([pred1[i], pred2[i], pred3[i]]))\n",
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362fbeb",
   "metadata": {},
   "source": [
    "Alternatively, you can use **“VotingClassifier”** module in sklearn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc9d3dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658536585365853"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9eac4",
   "metadata": {},
   "source": [
    "#### 2. Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690349f",
   "metadata": {},
   "source": [
    "Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. **Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de7ed1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f93d0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44495ba9",
   "metadata": {},
   "source": [
    "#### 3. Weighted Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769142e",
   "metadata": {},
   "source": [
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8762d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70c88ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalpred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60537f6",
   "metadata": {},
   "source": [
    "### 1).Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953557f",
   "metadata": {},
   "source": [
    "### 2).Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd463ffd",
   "metadata": {},
   "source": [
    "Stacking is a popular ensemble learning technique that involves combining multiple individual models to improve overall prediction performance. \n",
    "\n",
    "The basic idea behind stacking is to train several base models on the same dataset, then use their predictions as inputs to a meta-model. The meta-model can be trained on the same dataset or a different dataset, using the base model predictions as input features. Once the meta-model is trained, it can be used to predict the final outcome on new data.\n",
    "\n",
    "**Below is a step-wise explanation for a simple stacked ensemble**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d9491",
   "metadata": {},
   "source": [
    "1. The train set is split into 10 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c3d51",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-11-768x555.png\" width=\"35%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c49ec",
   "metadata": {},
   "source": [
    "2. A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0d76c",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-10-768x638.png\" width=\"30%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad862040",
   "metadata": {},
   "source": [
    "3. The base model (in this case, decision tree) is then fitted on the whole train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318e45a",
   "metadata": {},
   "source": [
    "4. Using this model, predictions are made on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582e175",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-2-768x577.png\" width=\"35%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffb336",
   "metadata": {},
   "source": [
    "5. Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce56a8b",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-3-768x573.png\" width=\"35%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67ce79",
   "metadata": {},
   "source": [
    "6. The predictions from the train set are used as features to build a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81214113",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image12.png\" width=\"25%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ca88b",
   "metadata": {},
   "source": [
    "7. This model is used to make final predictions on the test prediction set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7af5be",
   "metadata": {},
   "source": [
    "**Sample Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "643edddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f1bafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DATASET\n",
    "df = pd.read_csv(\"/Users/crystal/Desktop/Random Forest/heart.csv\")\n",
    "x = df.drop('target', axis = 1)\n",
    "y = df['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8e5de06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the stacked model: 0.9658536585365853\n"
     ]
    }
   ],
   "source": [
    "# Split train and test data into two parts\n",
    "train1, train2, y_train1, y_train2 = train_test_split(x_train, y_train, test_size=0.5, random_state=1)\n",
    "\n",
    "# Train and predict on the first base model\n",
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "model1.fit(train1, y_train1)\n",
    "train_pred1 = model1.predict_proba(train2)[:, 1]\n",
    "test_pred1 = model1.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Train and predict on the second base model\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(train1, y_train1)\n",
    "train_pred2 = model2.predict_proba(train2)[:, 1]\n",
    "test_pred2 = model2.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Only select rows that correspond to the same data points in train_pred1 and test_pred1\n",
    "train_pred2 = train_pred2[train2.index.isin(train1.index)]\n",
    "test_pred2 = test_pred2[x_test.index.isin(train1.index)]\n",
    "\n",
    "# Combine predictions from base models into a single dataframe\n",
    "train_pred1 = pd.DataFrame(train_pred1)\n",
    "train_pred2 = pd.DataFrame(train_pred2)\n",
    "test_pred1 = pd.DataFrame(test_pred1)\n",
    "test_pred2 = pd.DataFrame(test_pred2)\n",
    "\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "# Fill any missing values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "df_test.fillna(0, inplace=True)\n",
    "\n",
    "# Train a logistic regression model on the stacked predictions\n",
    "model = LogisticRegression()\n",
    "model.fit(df, y_train2)\n",
    "\n",
    "# Make predictions on the test set using the stacked model\n",
    "y_pred = model.predict(df_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy score of the stacked model:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21c16f",
   "metadata": {},
   "source": [
    "### 3).Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
