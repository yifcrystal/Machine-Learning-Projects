{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05941fc3",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272485a2",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "\n",
    "Word2Vec is a NLP model that under the assumption that if two words have similar neighbors then they are supposed to be similar in meanings or highly related. This codes demo shows how we can use Gensim implementation of Word2Vec. \n",
    "\n",
    "Tutorial: https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.ZAeKny1h2M4\n",
    "\n",
    "Replicate the process of generating words embeddings for IMDB Movie Reviews dataset, which includes text from 50k reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba5a1c",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f37d1e",
   "metadata": {},
   "source": [
    "Imports and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c0a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d050889",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "IMDB Movie Reviews dataset with text from 50k reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8eba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "input_file = 'IMDB Dataset.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "# Extract the review texts and sentiment labels\n",
    "reviews = df['review'].tolist()\n",
    "labels = df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b88978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9daa22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20e7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the reviews\n",
    "def read_input(input_file):\n",
    "    # Read in file avoid reading the column names\n",
    "    with open(input_file, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            yield gensim.utils.simple_preprocess(row['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898f4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import csv\n",
    "documents = list(read_input(input_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a24e77",
   "metadata": {},
   "source": [
    "**Training the Word2Vec model**\n",
    "\n",
    "In this process, we pass on a list of lists to the Word2Vec model, in which each list within the main list contains a set of tokens from a user review. Word2Vec will uses all tokens to create a vocabulary, in other words, a set of unique words.\n",
    "\n",
    "The main idea of this model training is to train a simple neural network with a single hidden layer. We are not using the neural network after traing and instead we want to learn the weights of the hidden layer, which refer to the word vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a980018",
   "metadata": {},
   "source": [
    "**Parameters Setup**\n",
    "\n",
    "- documents: This is the input corpus, which is a list of lists of words. Each sublist contains the words of a single document in the corpus.\n",
    "- vector_size: This parameter sets the size of the word vectors that will be produced by the Word2Vec model. \n",
    "- window: This parameter sets the maximum distance between the target word and its context words. In other words, it determines the size of the \"window\" of words that the model considers when learning the word embeddings. \n",
    "- min_count: This parameter sets the minimum frequency threshold for words to be included in the vocabulary. Words that occur less frequently than this threshold are discarded. \n",
    "- sg: This parameter sets the training algorithm to be used. sg stands for \"skip-gram\", which is a popular algorithm for training word embeddings. The other option is cbow, which stands for \"continuous bag-of-words\".\n",
    "    - **Skip-gram (sg)**: This algorithm aims to **predict the context words given a target word**. Specifically, it tries to maximize the probability of observing the context words given the target word. This means that the target word is used as input to the model, and the output is a probability distribution over the context words.\n",
    "    - **Continuous Bag-of-Words (CBOW)**: This algorithm is the opposite of the skip-gram algorithm. It aims to **predict the target word given a context of surrounding words**. Specifically, it tries to maximize the probability of observing the target word given the context words. This means that the context words are used as input to the model, and the output is a probability distribution over the target words.\n",
    "    - In other words, **the skip-gram model predicts the context words given a target word, while the CBOW model predicts the target word given a context of surrounding words. The skip-gram algorithm is typically better suited for larger datasets and infrequent words, while the CBOW algorithm can be faster and more accurate for frequent words and smaller datasets**.\n",
    "- workers: This parameter sets the number of threads to be used for training the model.\"threads\" refers to the number of independent computational processes that will be used to train the model. Specifically, setting the workers parameter to a value greater than 1 enables parallel processing of the training data, which can speed up the training process and reduce the overall training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b16a2",
   "metadata": {},
   "source": [
    "**Default Setup**: By default, size is set to 100, window to 5, min_count to 5, and sg to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6fadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 15:02:32,502 : INFO : collecting all words and their counts\n",
      "2023-04-01 15:02:32,509 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-04-01 15:02:33,250 : INFO : PROGRESS: at sentence #10000, processed 2235115 words, keeping 51730 word types\n",
      "2023-04-01 15:02:34,170 : INFO : PROGRESS: at sentence #20000, processed 4471355 words, keeping 68515 word types\n",
      "2023-04-01 15:02:34,822 : INFO : PROGRESS: at sentence #30000, processed 6703795 words, keeping 80657 word types\n",
      "2023-04-01 15:02:35,753 : INFO : PROGRESS: at sentence #40000, processed 8930547 words, keeping 90755 word types\n",
      "2023-04-01 15:02:36,579 : INFO : collected 99476 word types from a corpus of 11176467 raw words and 50000 sentences\n",
      "2023-04-01 15:02:36,580 : INFO : Creating a fresh vocabulary\n",
      "2023-04-01 15:02:36,832 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 39191 unique words (39.40% of original 99476, drops 60285)', 'datetime': '2023-04-01T15:02:36.793827', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-01 15:02:36,835 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 11077800 word corpus (99.12% of original 11176467, drops 98667)', 'datetime': '2023-04-01T15:02:36.835394', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-01 15:02:37,113 : INFO : deleting the raw counts dictionary of 99476 items\n",
      "2023-04-01 15:02:37,187 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2023-04-01 15:02:37,189 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8424982.10813893 word corpus (76.1%% of prior 11077800)', 'datetime': '2023-04-01T15:02:37.189913', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-01 15:02:37,584 : INFO : estimated required memory for 39191 words and 100 dimensions: 50948300 bytes\n",
      "2023-04-01 15:02:37,585 : INFO : resetting layer weights\n",
      "2023-04-01 15:02:37,618 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-01T15:02:37.618584', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-04-01 15:02:37,620 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 39191 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-01T15:02:37.620055', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-04-01 15:02:38,724 : INFO : EPOCH 0 - PROGRESS: at 3.60% examples, 295111 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:39,727 : INFO : EPOCH 0 - PROGRESS: at 7.36% examples, 307259 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:40,781 : INFO : EPOCH 0 - PROGRESS: at 11.45% examples, 317228 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:02:41,863 : INFO : EPOCH 0 - PROGRESS: at 15.87% examples, 323781 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:02:42,874 : INFO : EPOCH 0 - PROGRESS: at 19.98% examples, 326136 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:02:43,939 : INFO : EPOCH 0 - PROGRESS: at 23.98% examples, 326120 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:44,941 : INFO : EPOCH 0 - PROGRESS: at 26.74% examples, 312718 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:45,946 : INFO : EPOCH 0 - PROGRESS: at 30.37% examples, 311460 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:46,973 : INFO : EPOCH 0 - PROGRESS: at 32.84% examples, 299360 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:48,031 : INFO : EPOCH 0 - PROGRESS: at 35.07% examples, 286602 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:02:49,080 : INFO : EPOCH 0 - PROGRESS: at 37.54% examples, 278305 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:02:50,089 : INFO : EPOCH 0 - PROGRESS: at 40.15% examples, 273446 words/s, in_qsize 20, out_qsize 2\n",
      "2023-04-01 15:02:51,157 : INFO : EPOCH 0 - PROGRESS: at 42.27% examples, 265460 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:02:52,178 : INFO : EPOCH 0 - PROGRESS: at 45.37% examples, 264636 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:02:53,185 : INFO : EPOCH 0 - PROGRESS: at 48.81% examples, 266157 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:54,198 : INFO : EPOCH 0 - PROGRESS: at 52.04% examples, 266450 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:55,228 : INFO : EPOCH 0 - PROGRESS: at 55.28% examples, 265977 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:56,261 : INFO : EPOCH 0 - PROGRESS: at 58.26% examples, 264404 words/s, in_qsize 19, out_qsize 1\n",
      "2023-04-01 15:02:57,284 : INFO : EPOCH 0 - PROGRESS: at 61.72% examples, 265774 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:02:58,291 : INFO : EPOCH 0 - PROGRESS: at 65.20% examples, 267166 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:02:59,291 : INFO : EPOCH 0 - PROGRESS: at 68.55% examples, 267490 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:00,316 : INFO : EPOCH 0 - PROGRESS: at 71.68% examples, 267149 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:01,372 : INFO : EPOCH 0 - PROGRESS: at 74.79% examples, 266237 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:02,398 : INFO : EPOCH 0 - PROGRESS: at 77.71% examples, 265124 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:03,410 : INFO : EPOCH 0 - PROGRESS: at 80.32% examples, 263055 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:03:04,421 : INFO : EPOCH 0 - PROGRESS: at 83.59% examples, 263310 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:05,427 : INFO : EPOCH 0 - PROGRESS: at 86.50% examples, 263083 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:06,437 : INFO : EPOCH 0 - PROGRESS: at 89.50% examples, 262804 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:07,437 : INFO : EPOCH 0 - PROGRESS: at 92.77% examples, 263182 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:08,440 : INFO : EPOCH 0 - PROGRESS: at 95.90% examples, 263219 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:09,445 : INFO : EPOCH 0 - PROGRESS: at 99.32% examples, 263699 words/s, in_qsize 8, out_qsize 1\n",
      "2023-04-01 15:03:09,562 : INFO : EPOCH 0: training on 11176467 raw words (8424391 effective words) took 31.8s, 264538 effective words/s\n",
      "2023-04-01 15:03:10,662 : INFO : EPOCH 1 - PROGRESS: at 2.82% examples, 210176 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:11,697 : INFO : EPOCH 1 - PROGRESS: at 5.61% examples, 220245 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:12,760 : INFO : EPOCH 1 - PROGRESS: at 8.98% examples, 235331 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:13,774 : INFO : EPOCH 1 - PROGRESS: at 12.00% examples, 243684 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:14,800 : INFO : EPOCH 1 - PROGRESS: at 15.09% examples, 243882 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:15,816 : INFO : EPOCH 1 - PROGRESS: at 18.48% examples, 250407 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:16,893 : INFO : EPOCH 1 - PROGRESS: at 22.00% examples, 254033 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:17,973 : INFO : EPOCH 1 - PROGRESS: at 25.53% examples, 256615 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:19,048 : INFO : EPOCH 1 - PROGRESS: at 29.02% examples, 258675 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:20,132 : INFO : EPOCH 1 - PROGRESS: at 32.58% examples, 260261 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:21,172 : INFO : EPOCH 1 - PROGRESS: at 36.15% examples, 262491 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:22,181 : INFO : EPOCH 1 - PROGRESS: at 39.63% examples, 264839 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:23,181 : INFO : EPOCH 1 - PROGRESS: at 42.84% examples, 265453 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:03:24,211 : INFO : EPOCH 1 - PROGRESS: at 46.05% examples, 265482 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:25,244 : INFO : EPOCH 1 - PROGRESS: at 49.42% examples, 265950 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 15:03:26,268 : INFO : EPOCH 1 - PROGRESS: at 52.81% examples, 266542 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:03:27,297 : INFO : EPOCH 1 - PROGRESS: at 56.14% examples, 266974 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:28,352 : INFO : EPOCH 1 - PROGRESS: at 59.24% examples, 265449 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:29,380 : INFO : EPOCH 1 - PROGRESS: at 62.61% examples, 266312 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:30,397 : INFO : EPOCH 1 - PROGRESS: at 65.64% examples, 265745 words/s, in_qsize 16, out_qsize 3\n",
      "2023-04-01 15:03:31,401 : INFO : EPOCH 1 - PROGRESS: at 68.88% examples, 265761 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:32,460 : INFO : EPOCH 1 - PROGRESS: at 72.31% examples, 266073 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:33,462 : INFO : EPOCH 1 - PROGRESS: at 75.57% examples, 266460 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:34,497 : INFO : EPOCH 1 - PROGRESS: at 78.60% examples, 265545 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:35,568 : INFO : EPOCH 1 - PROGRESS: at 82.08% examples, 265685 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:36,571 : INFO : EPOCH 1 - PROGRESS: at 85.12% examples, 265406 words/s, in_qsize 20, out_qsize 2\n",
      "2023-04-01 15:03:37,577 : INFO : EPOCH 1 - PROGRESS: at 88.22% examples, 265600 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:38,583 : INFO : EPOCH 1 - PROGRESS: at 91.53% examples, 266097 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:39,594 : INFO : EPOCH 1 - PROGRESS: at 94.61% examples, 265735 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:03:40,609 : INFO : EPOCH 1 - PROGRESS: at 97.87% examples, 265786 words/s, in_qsize 19, out_qsize 2\n",
      "2023-04-01 15:03:41,083 : INFO : EPOCH 1: training on 11176467 raw words (8426487 effective words) took 31.5s, 267385 effective words/s\n",
      "2023-04-01 15:03:42,147 : INFO : EPOCH 2 - PROGRESS: at 2.80% examples, 217716 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:43,259 : INFO : EPOCH 2 - PROGRESS: at 6.29% examples, 243698 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:44,349 : INFO : EPOCH 2 - PROGRESS: at 9.78% examples, 252849 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:45,471 : INFO : EPOCH 2 - PROGRESS: at 13.23% examples, 256008 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:46,607 : INFO : EPOCH 2 - PROGRESS: at 16.78% examples, 256760 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:03:47,607 : INFO : EPOCH 2 - PROGRESS: at 19.42% examples, 251496 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:48,638 : INFO : EPOCH 2 - PROGRESS: at 22.40% examples, 251380 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:49,673 : INFO : EPOCH 2 - PROGRESS: at 25.38% examples, 249578 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:50,718 : INFO : EPOCH 2 - PROGRESS: at 28.13% examples, 247008 words/s, in_qsize 17, out_qsize 2\n",
      "2023-04-01 15:03:51,835 : INFO : EPOCH 2 - PROGRESS: at 30.82% examples, 242061 words/s, in_qsize 20, out_qsize 4\n",
      "2023-04-01 15:03:53,072 : INFO : EPOCH 2 - PROGRESS: at 33.46% examples, 235673 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:54,098 : INFO : EPOCH 2 - PROGRESS: at 35.96% examples, 233002 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:55,128 : INFO : EPOCH 2 - PROGRESS: at 37.90% examples, 227381 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:56,212 : INFO : EPOCH 2 - PROGRESS: at 39.99% examples, 222762 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:03:57,267 : INFO : EPOCH 2 - PROGRESS: at 42.56% examples, 221953 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:03:58,354 : INFO : EPOCH 2 - PROGRESS: at 44.75% examples, 218721 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:03:59,376 : INFO : EPOCH 2 - PROGRESS: at 46.93% examples, 216619 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:00,377 : INFO : EPOCH 2 - PROGRESS: at 49.42% examples, 216150 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:01,397 : INFO : EPOCH 2 - PROGRESS: at 51.94% examples, 215918 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:02,413 : INFO : EPOCH 2 - PROGRESS: at 54.35% examples, 214678 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:03,465 : INFO : EPOCH 2 - PROGRESS: at 56.70% examples, 213543 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:04,557 : INFO : EPOCH 2 - PROGRESS: at 59.34% examples, 212780 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:05,668 : INFO : EPOCH 2 - PROGRESS: at 61.89% examples, 212219 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:06,702 : INFO : EPOCH 2 - PROGRESS: at 64.40% examples, 212054 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:07,758 : INFO : EPOCH 2 - PROGRESS: at 67.28% examples, 212552 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:08,805 : INFO : EPOCH 2 - PROGRESS: at 70.10% examples, 213065 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:09,826 : INFO : EPOCH 2 - PROGRESS: at 72.64% examples, 212962 words/s, in_qsize 15, out_qsize 4\n",
      "2023-04-01 15:04:10,837 : INFO : EPOCH 2 - PROGRESS: at 75.83% examples, 214762 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:11,842 : INFO : EPOCH 2 - PROGRESS: at 78.60% examples, 215235 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:12,908 : INFO : EPOCH 2 - PROGRESS: at 81.54% examples, 215697 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:13,914 : INFO : EPOCH 2 - PROGRESS: at 84.61% examples, 216981 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:14,919 : INFO : EPOCH 2 - PROGRESS: at 87.54% examples, 218140 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:15,939 : INFO : EPOCH 2 - PROGRESS: at 90.60% examples, 219169 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:16,952 : INFO : EPOCH 2 - PROGRESS: at 93.66% examples, 220193 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:17,955 : INFO : EPOCH 2 - PROGRESS: at 96.63% examples, 220975 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:18,930 : INFO : EPOCH 2: training on 11176467 raw words (8424678 effective words) took 37.8s, 222646 effective words/s\n",
      "2023-04-01 15:04:19,939 : INFO : EPOCH 3 - PROGRESS: at 2.72% examples, 222246 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:21,042 : INFO : EPOCH 3 - PROGRESS: at 5.46% examples, 215715 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:22,045 : INFO : EPOCH 3 - PROGRESS: at 8.80% examples, 236997 words/s, in_qsize 18, out_qsize 0\n",
      "2023-04-01 15:04:23,155 : INFO : EPOCH 3 - PROGRESS: at 11.45% examples, 230732 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:24,158 : INFO : EPOCH 3 - PROGRESS: at 14.81% examples, 240343 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:25,231 : INFO : EPOCH 3 - PROGRESS: at 17.62% examples, 237000 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:26,414 : INFO : EPOCH 3 - PROGRESS: at 21.14% examples, 239019 words/s, in_qsize 17, out_qsize 2\n",
      "2023-04-01 15:04:27,527 : INFO : EPOCH 3 - PROGRESS: at 24.66% examples, 242503 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:04:28,541 : INFO : EPOCH 3 - PROGRESS: at 27.97% examples, 246162 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:29,583 : INFO : EPOCH 3 - PROGRESS: at 30.82% examples, 244368 words/s, in_qsize 20, out_qsize 2\n",
      "2023-04-01 15:04:30,718 : INFO : EPOCH 3 - PROGRESS: at 34.34% examples, 246014 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:31,815 : INFO : EPOCH 3 - PROGRESS: at 37.90% examples, 247961 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:32,875 : INFO : EPOCH 3 - PROGRESS: at 41.17% examples, 249175 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:33,916 : INFO : EPOCH 3 - PROGRESS: at 44.09% examples, 248180 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:34,925 : INFO : EPOCH 3 - PROGRESS: at 47.38% examples, 250154 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:36,033 : INFO : EPOCH 3 - PROGRESS: at 50.22% examples, 247773 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:37,036 : INFO : EPOCH 3 - PROGRESS: at 52.90% examples, 246351 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:38,043 : INFO : EPOCH 3 - PROGRESS: at 55.88% examples, 246565 words/s, in_qsize 20, out_qsize 4\n",
      "2023-04-01 15:04:39,074 : INFO : EPOCH 3 - PROGRESS: at 58.35% examples, 243921 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:40,131 : INFO : EPOCH 3 - PROGRESS: at 61.00% examples, 242615 words/s, in_qsize 20, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 15:04:41,136 : INFO : EPOCH 3 - PROGRESS: at 64.20% examples, 243980 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:42,157 : INFO : EPOCH 3 - PROGRESS: at 66.48% examples, 241244 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:04:43,195 : INFO : EPOCH 3 - PROGRESS: at 68.97% examples, 239483 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:44,196 : INFO : EPOCH 3 - PROGRESS: at 71.85% examples, 239634 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:45,313 : INFO : EPOCH 3 - PROGRESS: at 74.34% examples, 237387 words/s, in_qsize 20, out_qsize 3\n",
      "2023-04-01 15:04:46,321 : INFO : EPOCH 3 - PROGRESS: at 77.27% examples, 237617 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:04:47,446 : INFO : EPOCH 3 - PROGRESS: at 79.71% examples, 235291 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:48,473 : INFO : EPOCH 3 - PROGRESS: at 82.23% examples, 234336 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:49,491 : INFO : EPOCH 3 - PROGRESS: at 84.77% examples, 233577 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:50,542 : INFO : EPOCH 3 - PROGRESS: at 86.58% examples, 230923 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:04:51,604 : INFO : EPOCH 3 - PROGRESS: at 89.14% examples, 230167 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:52,661 : INFO : EPOCH 3 - PROGRESS: at 91.10% examples, 227797 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:53,706 : INFO : EPOCH 3 - PROGRESS: at 93.57% examples, 226901 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:54,724 : INFO : EPOCH 3 - PROGRESS: at 96.18% examples, 226598 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:04:55,767 : INFO : EPOCH 3 - PROGRESS: at 98.87% examples, 226191 words/s, in_qsize 13, out_qsize 0\n",
      "2023-04-01 15:04:56,082 : INFO : EPOCH 3: training on 11176467 raw words (8424652 effective words) took 37.1s, 226817 effective words/s\n",
      "2023-04-01 15:04:57,100 : INFO : EPOCH 4 - PROGRESS: at 1.58% examples, 132400 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:58,102 : INFO : EPOCH 4 - PROGRESS: at 2.91% examples, 118195 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:04:59,390 : INFO : EPOCH 4 - PROGRESS: at 4.59% examples, 115144 words/s, in_qsize 19, out_qsize 3\n",
      "2023-04-01 15:05:00,402 : INFO : EPOCH 4 - PROGRESS: at 5.95% examples, 115782 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:01,411 : INFO : EPOCH 4 - PROGRESS: at 6.46% examples, 102256 words/s, in_qsize 14, out_qsize 5\n",
      "2023-04-01 15:05:02,708 : INFO : EPOCH 4 - PROGRESS: at 8.09% examples, 102392 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:03,792 : INFO : EPOCH 4 - PROGRESS: at 9.77% examples, 107065 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:04,885 : INFO : EPOCH 4 - PROGRESS: at 11.45% examples, 110651 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:05,915 : INFO : EPOCH 4 - PROGRESS: at 13.23% examples, 114161 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:07,387 : INFO : EPOCH 4 - PROGRESS: at 15.89% examples, 118938 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:08,452 : INFO : EPOCH 4 - PROGRESS: at 17.90% examples, 122455 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:09,509 : INFO : EPOCH 4 - PROGRESS: at 19.70% examples, 123833 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:10,550 : INFO : EPOCH 4 - PROGRESS: at 22.00% examples, 128729 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:11,550 : INFO : EPOCH 4 - PROGRESS: at 24.28% examples, 132814 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:12,634 : INFO : EPOCH 4 - PROGRESS: at 25.80% examples, 131757 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:05:13,697 : INFO : EPOCH 4 - PROGRESS: at 27.27% examples, 130922 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:14,765 : INFO : EPOCH 4 - PROGRESS: at 28.84% examples, 130579 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:15,850 : INFO : EPOCH 4 - PROGRESS: at 30.65% examples, 130922 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:05:16,883 : INFO : EPOCH 4 - PROGRESS: at 32.03% examples, 130133 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:17,895 : INFO : EPOCH 4 - PROGRESS: at 34.08% examples, 131915 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:18,901 : INFO : EPOCH 4 - PROGRESS: at 35.18% examples, 129996 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:19,918 : INFO : EPOCH 4 - PROGRESS: at 36.85% examples, 130347 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:21,022 : INFO : EPOCH 4 - PROGRESS: at 38.63% examples, 130456 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:22,167 : INFO : EPOCH 4 - PROGRESS: at 39.81% examples, 128684 words/s, in_qsize 19, out_qsize 1\n",
      "2023-04-01 15:05:23,192 : INFO : EPOCH 4 - PROGRESS: at 41.08% examples, 127890 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:24,306 : INFO : EPOCH 4 - PROGRESS: at 42.81% examples, 128100 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:25,386 : INFO : EPOCH 4 - PROGRESS: at 44.84% examples, 129182 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:26,532 : INFO : EPOCH 4 - PROGRESS: at 46.50% examples, 128941 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:27,626 : INFO : EPOCH 4 - PROGRESS: at 47.93% examples, 128263 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:28,647 : INFO : EPOCH 4 - PROGRESS: at 49.43% examples, 128101 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:05:29,724 : INFO : EPOCH 4 - PROGRESS: at 51.10% examples, 128191 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:30,744 : INFO : EPOCH 4 - PROGRESS: at 52.81% examples, 128482 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:31,762 : INFO : EPOCH 4 - PROGRESS: at 54.76% examples, 129400 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:32,776 : INFO : EPOCH 4 - PROGRESS: at 56.60% examples, 130075 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:33,788 : INFO : EPOCH 4 - PROGRESS: at 58.45% examples, 130522 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:34,911 : INFO : EPOCH 4 - PROGRESS: at 59.16% examples, 128286 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:35,915 : INFO : EPOCH 4 - PROGRESS: at 60.60% examples, 128217 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:36,983 : INFO : EPOCH 4 - PROGRESS: at 61.99% examples, 127784 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:38,081 : INFO : EPOCH 4 - PROGRESS: at 63.52% examples, 127619 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:39,252 : INFO : EPOCH 4 - PROGRESS: at 65.19% examples, 127416 words/s, in_qsize 20, out_qsize 2\n",
      "2023-04-01 15:05:40,493 : INFO : EPOCH 4 - PROGRESS: at 67.02% examples, 127201 words/s, in_qsize 20, out_qsize 1\n",
      "2023-04-01 15:05:41,541 : INFO : EPOCH 4 - PROGRESS: at 69.27% examples, 128513 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:42,637 : INFO : EPOCH 4 - PROGRESS: at 70.63% examples, 127849 words/s, in_qsize 19, out_qsize 2\n",
      "2023-04-01 15:05:43,651 : INFO : EPOCH 4 - PROGRESS: at 72.57% examples, 128553 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:44,677 : INFO : EPOCH 4 - PROGRESS: at 74.52% examples, 129224 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:45,679 : INFO : EPOCH 4 - PROGRESS: at 77.17% examples, 131116 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:46,733 : INFO : EPOCH 4 - PROGRESS: at 79.96% examples, 132938 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:05:47,733 : INFO : EPOCH 4 - PROGRESS: at 82.91% examples, 135082 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:48,789 : INFO : EPOCH 4 - PROGRESS: at 85.45% examples, 136591 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:49,961 : INFO : EPOCH 4 - PROGRESS: at 88.06% examples, 137839 words/s, in_qsize 17, out_qsize 2\n",
      "2023-04-01 15:05:50,966 : INFO : EPOCH 4 - PROGRESS: at 90.84% examples, 139626 words/s, in_qsize 17, out_qsize 3\n",
      "2023-04-01 15:05:51,973 : INFO : EPOCH 4 - PROGRESS: at 93.37% examples, 140942 words/s, in_qsize 20, out_qsize 0\n",
      "2023-04-01 15:05:52,994 : INFO : EPOCH 4 - PROGRESS: at 95.82% examples, 142044 words/s, in_qsize 19, out_qsize 0\n",
      "2023-04-01 15:05:54,022 : INFO : EPOCH 4 - PROGRESS: at 98.24% examples, 142946 words/s, in_qsize 18, out_qsize 1\n",
      "2023-04-01 15:05:54,605 : INFO : EPOCH 4: training on 11176467 raw words (8426651 effective words) took 58.5s, 144020 effective words/s\n",
      "2023-04-01 15:05:54,606 : INFO : Word2Vec lifecycle event {'msg': 'training on 55882335 raw words (42126859 effective words) took 197.0s, 213862 effective words/s', 'datetime': '2023-04-01T15:05:54.606473', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 15:05:54,607 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=39191, vector_size=100, alpha=0.025>', 'datetime': '2023-04-01T15:05:54.607167', 'gensim': '4.3.1', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(documents,vector_size=100,window=5,min_count=5,sg = 1,workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ecf31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty numpy array for reviews embeddings\n",
    "import numpy as np\n",
    "reviews_embeddings = np.zeros((len(documents), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c86076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each review\n",
    "for i, review in enumerate(documents):\n",
    "    words_embeddings = []\n",
    "    # loop through each word in the review\n",
    "    for word in review:\n",
    "        # if the word is in the model vocabulary, append its embedding to words_embeddings\n",
    "        if word in model.wv.key_to_index:\n",
    "            words_embeddings.append(model.wv[word])\n",
    "    # calculate the mean embedding for the review and store it in reviews_embeddings\n",
    "    if words_embeddings:\n",
    "        mean_embedding = np.mean(words_embeddings, axis=0)\n",
    "        reviews_embeddings[i] = mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8420af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.869975\n",
      "Test accuracy: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Use reviews_embeddings as the input to logistic regression to classify each review.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train_acc = lr.score(X_train, y_train)\n",
    "test_acc = lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91c6be",
   "metadata": {},
   "source": [
    "Now I will vary size between [25, 50, 100, 150] and plot size vs. train and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b93b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store accuracies\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Define vector sizes to test\n",
    "vector_sizes = [25, 50, 100, 150]\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    # Build vocabulary and train model\n",
    "    model = gensim.models.Word2Vec(documents,vector_size=vector_size,window=5,min_count=5,sg = 1,workers=10)\n",
    "    \n",
    "    # Create empty numpy array for reviews embeddings\n",
    "    reviews_embeddings = np.zeros((len(documents), vector_size))\n",
    "    \n",
    "    # Loop through each review\n",
    "    for i, review in enumerate(documents):\n",
    "        words_embeddings = []\n",
    "        # loop through each word in the review\n",
    "        for word in review:\n",
    "            # if the word is in the model vocabulary, append its embedding to words_embeddings\n",
    "            if word in model.wv.key_to_index:\n",
    "                words_embeddings.append(model.wv[word])\n",
    "        # calculate the mean embedding for the review and store it in reviews_embeddings\n",
    "        if words_embeddings:\n",
    "            mean_embedding = np.mean(words_embeddings, axis=0)\n",
    "            reviews_embeddings[i] = mean_embedding\n",
    "    \n",
    "    # Use reviews_embeddings as the input to logistic regression to classify each review.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reviews_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = lr.score(X_train, y_train)\n",
    "    test_acc = lr.score(X_test, y_test)\n",
    "    \n",
    "    # Append accuracies to lists\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size vs. train and test accuracy\n",
    "plt.plot(vector_sizes, train_accs, label='Train Accuracy')\n",
    "plt.plot(vector_sizes, test_accs, label='Test Accuracy')\n",
    "plt.xlabel('Vector Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3e30c",
   "metadata": {},
   "source": [
    "Now I will vary the window between [2, 3, 5, 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
